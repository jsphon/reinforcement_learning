Q Trainer

 - Accepts experience of (old state, action, reward, new_state)
 - Trains onto target reward + gamma * maxQ
  where maxQ = max(Q(new_state))

 - For better vectorisation, we could have as our experience
  - (old state, rewards, new_states), where rewards and new_states are
   lists with one value per action.
  - thus, calling Q(new_states) will give a rectangular result of
    shape (num_actions, num_actions), and we take the max over one of the
    axis to give a vectorised
     rewards + gamma * maxQs
    for training

= Reinforcement Learning System =

 - Contains the elements of reinforcement learning:

== Model ==

 - Maps (old_state, action) to new_state
 - May or may not be deterministic

=== Example Model: Grid ===

 - Maps (position, actions) to new_position

== Reward Function ==

 - Maps (old_state, action, new_state) to a reward

=== Reward Function Examples ===
 - Grid World maps the new state to a reward
 - Trading sim maps the change in price, and prior position, to a reward. i.e. uses prior and current state

== Policy ==

 - How the agent reacts to states.
 i.e. maps states to actions

== Value Function ==

 - Calculates the value of each action that a state can take.

Defn: Action/Reward Vector

Suppose we have n possible actions. Then the action/reward vector a\in\R^n is a vector where
a[i] is the reward from action a.

== Live ==

Can only step through time in one direction, taking one action at each point in time. This is how an
online system would learn.

i.e.

action = Policy(current_state)
new_state = Model(current_state, action)
reward = Reward Function(state, action, new_state)
repeat

This method leaves a lot of unhelpful values in the action/reward vector.

= Experience Generator =

 - Used to generate lots of experience.
 - It can follow a random policy, which will save us calling the value function
 - It can use action rewards to explore the state space, which will save us calling the value function
 - It can use the value function to choose a policy

== Model Based Experience Generator ==

Given a state, gather experience for each action.

i.e.

for each action:
    new_state = Model(current_state, action)
    reward = Reward Function(state, action, new_state

new_state = random choice of new states

= Training Algorithm / Trainer=

 - Contains a system(s) and tries to train it
 - Uses an experience generator to generate experience
 - Can the experience generator be live?

== Live Trainer ==

 Step through live model.
 At each step, update the q function.

class Experience

 - History of (old_state, action, reward, new_state)

class Episode

 - A game from beginning to end
 - Contains a list of state, action, reward

class XXXEpisode

 - Also contains action_states, which is the "next state" for each action at every epoch
   i.e. state array of shape [num_epochs, num_actions]
   These forward looking states are used by the value function to calculate the target values
   of a given action i.e.
   target value = reward + value_function(next_state)


class State

 - The state of the model
 - Represented as a numpy array. External interface of this must be 1d numpy array
   which must be compatible with the system's

 = Examples =
  - Grid world
   - fixed initial state
   - random initial state
   - stochastic wind
